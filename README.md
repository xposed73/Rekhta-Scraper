### Dataset to JSON
```py
import os
import json

# Path to the folder containing the text files
folder_path = 'urdu'

# List to store category dictionaries
categories = []

# Iterate over each file in the folder
for file_name in os.listdir(folder_path):
    file_path = os.path.join(folder_path, file_name)
    # Check if the path is a file (not a directory)
    if os.path.isfile(file_path):
        # Read the content of the file
        with open(file_path, 'r', encoding='utf-8') as file:
            lines = file.readlines()
            # Extract category name from the first line
            category_name = lines[1].strip()
            # Remove category name from the lines
            content_lines = lines[1:]
            poetry_lines = []
            # Group content lines into pairs (line1, line2)
            for i in range(0, len(content_lines), 2):
                line1 = content_lines[i].strip()
                line2 = content_lines[i + 1].strip() if i + 1 < len(content_lines) else ''
                poetry_lines.append({"line1": line1, "line2": line2})
            # Create category dictionary
            category_dict = {
                "category_name": category_name,
                "poetry_lines": poetry_lines
            }
            categories.append(category_dict)

# Path to the output JSON file
output_file = 'output.json'

# Write categories to the output JSON file
with open(output_file, 'w', encoding='utf-8') as json_file:
    json.dump(categories, json_file, ensure_ascii=False, indent=4)

print("Data saved successfully to", output_file)
```
### Output JSON Sample
```json
[
    {
        "category_name": "Ø³Ø§Ù…Ù†Û’ Ø§Ø³ Ú©Û’ Ú©Ø¨Ú¾ÛŒ Ø§Ø³ Ú©ÛŒ Ø³ØªØ§Ø¦Ø´ Ù†ÛÛŒÚº Ú©ÛŒ",
        "poetry_lines": [
            {
                "line1": "Ø³Ø§Ù…Ù†Û’ Ø§Ø³ Ú©Û’ Ú©Ø¨Ú¾ÛŒ Ø§Ø³ Ú©ÛŒ Ø³ØªØ§Ø¦Ø´ Ù†ÛÛŒÚº Ú©ÛŒ",
                "line2": "Ø¯Ù„ Ù†Û’ Ú†Ø§ÛØ§ Ø¨Ú¾ÛŒ Ø§Ú¯Ø± ÛÙˆÙ†Ù¹ÙˆÚº Ù†Û’ Ø¬Ù†Ø¨Ø´ Ù†ÛÛŒÚº Ú©ÛŒ"
            },
            {
                "line1": "Ø§ÛÙ„ Ù…Ø­ÙÙ„ Ù¾Û Ú©Ø¨ Ø§Ø­ÙˆØ§Ù„ Ú©Ú¾Ù„Ø§ ÛÛ’ Ø§Ù¾Ù†Ø§",
                "line2": "Ù…ÛŒÚº Ø¨Ú¾ÛŒ Ø®Ø§Ù…ÙˆØ´ Ø±ÛØ§ Ø§Ø³ Ù†Û’ Ø¨Ú¾ÛŒ Ù¾Ø±Ø³Ø´ Ù†ÛÛŒÚº Ú©ÛŒ"
            },
            {
                "line1": "Ø¬Ø³ Ù‚Ø¯Ø± Ø§Ø³ Ø³Û’ ØªØ¹Ù„Ù‚ ØªÚ¾Ø§ Ú†Ù„Ø§ Ø¬Ø§ØªØ§ ÛÛ’",
                "line2": "Ø§Ø³ Ú©Ø§ Ú©ÛŒØ§ Ø±Ù†Ø¬ ÛÙˆ Ø¬Ø³ Ú©ÛŒ Ú©Ø¨Ú¾ÛŒ Ø®ÙˆØ§ÛØ´ Ù†ÛÛŒÚº Ú©ÛŒ"
            },
            {
                "line1": "ÛŒÛ Ø¨Ú¾ÛŒ Ú©ÛŒØ§ Ú©Ù… ÛÛ’ Ú©Û Ø¯ÙˆÙ†ÙˆÚº Ú©Ø§ Ø¨Ú¾Ø±Ù… Ù‚Ø§Ø¦Ù… ÛÛ’",
                "line2": "Ø§Ø³ Ù†Û’ Ø¨Ø®Ø´Ø´ Ù†ÛÛŒÚº Ú©ÛŒ ÛÙ… Ù†Û’ Ú¯Ø²Ø§Ø±Ø´ Ù†ÛÛŒÚº Ú©ÛŒ"
            },
            {
                "line1": "Ø§Ú© ØªÙˆ ÛÙ… Ú©Ùˆ Ø§Ø¯Ø¨ Ø¢Ø¯Ø§Ø¨ Ù†Û’ Ù¾ÛŒØ§Ø³Ø§ Ø±Ú©Ú¾Ø§",
                "line2": "Ø§Ø³ Ù¾Û Ù…Ø­ÙÙ„ Ù…ÛŒÚº ØµØ±Ø§Ø­ÛŒ Ù†Û’ Ø¨Ú¾ÛŒ Ú¯Ø±Ø¯Ø´ Ù†ÛÛŒÚº Ú©ÛŒ"
            },
            {
                "line1": "ÛÙ… Ú©Û Ø¯Ú©Ú¾ Ø§ÙˆÚ‘Ú¾ Ú©Û’ Ø®Ù„ÙˆØª Ù…ÛŒÚº Ù¾Ú‘Û’ Ø±ÛØªÛ’ ÛÛŒÚº",
                "line2": "ÛÙ… Ù†Û’ Ø¨Ø§Ø²Ø§Ø± Ù…ÛŒÚº Ø²Ø®Ù…ÙˆÚº Ú©ÛŒ Ù†Ù…Ø§Ø¦Ø´ Ù†ÛÛŒÚº Ú©ÛŒ"
            },
            {
                "line1": "Ø§Û’ Ù…Ø±Û’ Ø§Ø¨Ø± Ú©Ø±Ù… Ø¯ÛŒÚ©Ú¾ ÛŒÛ ÙˆÛŒØ±Ø§Ù†Û‚ Ø¬Ø§Úº",
                "line2": "Ú©ÛŒØ§ Ú©Ø³ÛŒ Ø¯Ø´Øª Ù¾Û ØªÙˆ Ù†Û’ Ú©Ø¨Ú¾ÛŒ Ø¨Ø§Ø±Ø´ Ù†ÛÛŒÚº Ú©ÛŒ"
            },
            {
                "line1": "Ú©Ù¹ Ù…Ø±Û’ Ø§Ù¾Ù†Û’ Ù‚Ø¨ÛŒÙ„Û’ Ú©ÛŒ Ø­ÙØ§Ø¸Øª Ú©Û’ Ù„ÛŒÛ’",
                "line2": "Ù…Ù‚ØªÙ„ Ø´ÛØ± Ù…ÛŒÚº Ù¹Ú¾ÛØ±Û’ Ø±ÛÛ’ Ø¬Ù†Ø¨Ø´ Ù†ÛÛŒÚº Ú©ÛŒ"
            },
            {
                "line1": "ÙˆÛ ÛÙ…ÛŒÚº Ø¨Ú¾ÙˆÙ„ Ú¯ÛŒØ§ ÛÙˆ ØªÙˆ Ø¹Ø¬Ø¨ Ú©ÛŒØ§ ÛÛ’ ÙØ±Ø§Ø²Ø”",
                "line2": "ÛÙ… Ù†Û’ Ø¨Ú¾ÛŒ Ù…ÛŒÙ„ Ù…Ù„Ø§Ù‚Ø§Øª Ú©ÛŒ Ú©ÙˆØ´Ø´ Ù†ÛÛŒÚº Ú©ÛŒ"
            }
        ]
    }
]
```

### Page Scraper
```py
import requests
from bs4 import BeautifulSoup

# Provide the URL of the webpage you want to scrape
url = "https://www.rekhta.org/ghazals/sitaaron-se-aage-jahaan-aur-bhii-hain-allama-iqbal-ghazals-1?lang=ur"

# Send a GET request to the URL
response = requests.get(url)

# Check if the request was successful (status code 200)
if response.status_code == 200:
    # Parse the HTML content of the webpage
    soup = BeautifulSoup(response.content, 'html.parser')
    
    # Select the elements you want to extract using CSS selectors
    # For example, select all paragraph elements
    paragraphs = soup.select('.c p')
    
    # Iterate over the selected paragraphs and print their text
    for paragraph in paragraphs:
        print(paragraph.get_text(strip=True))
else:
    print("Failed to retrieve the webpage. Status code:", response.status_code)
```

## ðŸ™ Support My Work

If you find this project helpful, consider supporting it by donating via UPI.

![Donate via UPI](https://raw.githubusercontent.com/xposed73/YTDL-python/main/upi-donation.png)

Thank you for your support! â¤ï¸

